# Description Based Text Classification with Reinforcement Learning

##### 发布时间：4 Jun 2020

---

- **问题：** 传统的文本分类分为两个步骤，特征提取和分类。
  
- **不足：** 传统方法只是将分类标签当成简单的标签，没有对于分类的具体描述。模型并不知道分类的描述。
- **改进：** 将类别和类别的描述信息结合。

- **问题：** 如何生成描述信息？

- **传统方法：** 人工模板

- **不足：** 费力，模型对于描述是如何构建的很敏感，人工模板不是很好的选择。
- **改进：** 通过强化学习来自动生成描述信息，（1）直接截取，（2）抽象抽取

- **补充：** 将NLP问题看成QA问题来处理。
  
- **三种生成描述的策略：** 
  - *模板：* 最直接生成标签描述的就是人工模板，模板的来源有很多，比如Wikipedia或者人工标注。
  - *截取：* 
  - *抽象：*

- **使用的数据集：**
  - *单标签分类：* AGNews,20newsgroups,DBPedia,Yahoo!,Answers,Yelp Review Polarity, IMDB.
  
  - *多标签分类：* Reuters,AAPD
  
  - *多方面情感分析：* BeerAdvocate, TripAdvisor



## Generative and Descriminative Text Classification with Recurrent Neural Networks

##### 发布时间：26 May 2017

---

- **不足:** 神经网络需要大量的数据并且当数据分布发生变化时表现就不行了。

- **发现：** 虽然以RNN为基础的判别模型比以RNN为基础的生成模型具有更高的渐进错误率，但是生成模型比判别模型收敛的更快。   所以，文章作者假设并发现了RNN为基础的生成模型对数据分布发生变化更具有健壮性。

- **解决方法：** 使用生成模型在复杂样本中取得提升，并提升适应数据变化的能力。

- **实验：** 使用单任务持续学习(标签是顺序给出的，在训练过程中会有新的标签出现，使得标签的分布发生变化)

- **实验结果：**

  - *判别模型：* 对之前的信息遗忘的很快（模型需要再次训练来适应新引用的标签，从而遗失了之前训练的有用信息）。
  
    - 推断其原因是因为，引入了新的标签后，模型需要调整其它的参数来使得预测新类型的可能尽可能地大。 这样会导致在原有标签分布下训练出的参数产生较大的变化。使得之前训练的信息丢失。
  
    > ​				注意：理论上来说是可以找到一个刚刚好的学习率来避免这种遗忘的，不过得到这个学习率需要对新引入的类有一个很好的‘了解’。  一个有希望的方法是弹性权重融合，不过这种方法需要计算Fisher信息矩阵。这对于复杂的矩阵很困难。
  
  - *生成模型：* 对于这种变化适应能力更强，能够更轻松的将新标签和其它标签分辨开。
  
    - 对于生成模型，每次当有一个新标签引入的时候只需要争对新的标签学习一个新的模型。	
    
      <u>关于生成模型，还看的不是很明白</u>
    



# Integrating Semantic Knowledge to Tackle Zero-shot Text Classification

##### 发布时间：29 Mar 2019

---

- **问题：** 很多分类方法只在传统的分类任务上有效，对于会出现新的类别的动态，开放的环境就不怎么行了。  
1. *零样本学习：* 零样本学习就是描述这样的场景

2. 零样本学习需要‘把握’有用的语义信息 （比如，类别的描述，类别之间的关系，和外部领域的知识）---利用已有的类的知识去理解未见过的类。

3. 目前在零样本学习中有三种常用的方法，但是目前很少有人去扩展他们，甚至没有人考虑将它们融合起来。并且之前的一些工作，他们使用的训练集和测试集的类别有一些相似性。

    | 三种常用方法                                    |     |
    | ----------------------------------------------- | --- |
    | 视觉概念（颜色，形状）和语义特征（动作，行为）  |     |
    | 类层级关系和知识图谱  ---表示类和特征之间的关系 |     |
    | 词嵌入--通过大量的语料库训练得到单词之间的关系  |     |

- **解决方法：** 作者提出了一种两阶段框架去共同进行数据增强和特征增强。其中融合了词嵌入，类别描述，类层级关系和知识图谱四种语义知识来帮助认识新出现的分类。    

  - 两个阶段都是基于CNN，第一个阶段叫做粗粒度分类，用于判断记录是出现过的还是未出现过的。第二个阶段叫做细粒度分析，用于最终得到记录的类别。
  - 框架中所有的分类器都只使用带有标记的出现过的类进行训练。

- **主要贡献：**
  
  - 提出了一个两阶段框架来处理零样本问题，框架不需要新出现的类别和已有的类别有相似性。
  - 提出了一个新的数据增强技术，叫做话题翻译（topic translatioon)。
- 提出了一个新的使用整体语义信息去进行特征增强的方法。                                                                                                                                                        
- **模型概述：**

  - 粗粒度分类：预测记录是来自出现过的类别还是未出现过的类别
    - 这里使用了一种数据增强方法来帮助分类器注意到未出现过的类别并且不接触它们真实的数据。

    - 使用CNN来预测记录属于每个已知分类的可能性，损失函数使用交叉熵损失函数，训练集中出现的所有类别作为正类，其余的作为反类。当有一个已知分类的可能性大于t时就说明记录属于已知分类。t是通过[Shu et al.,2017](https://arxiv.org/pdf/1709.08716.pdf)中的门限适应算法得到的。
    
    - <u>这里为什么要使用数据增强的原因我有点看没懂</u>，论文中说的是，在粗分类阶段，分类器只是用可见类中的负样本，所以他们可能无法区分出未见类中的正样本。所以当未见类在上述阶段中出现时，就尝试通过数据增强向分类器“介绍”它们。以便分类器能够拒绝相似的未见类样例。
    - 数据增强的方法是利用类比的方法将源已见类记录翻译的一个未见类记录。

  - 细粒度分类：确定记录的最终类别

    - 在第一阶段获得的粗粒度预测上使用传统分类器或者零样本分类器，这里会使用基于语义知识的特征增强来提供将记录和未出现的类别联系起来的额外信息来概括零样本推断。
- **相关工作：**
    零样本分类，NLP中的数据增强，NLP中的特征增强，联结和特征工作。


